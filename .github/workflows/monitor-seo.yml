name: Monitor SEO Multi-Sites

on:
  schedule:
    - cron: '0 8,20 * * *'
  workflow_dispatch:
    inputs:
      site_filter:
        description: 'Nom du site Ã  tester (vide = tous)'
        required: false
        default: ''
      force_alert:
        description: 'Forcer envoi email'
        required: false
        default: 'false'

env:
  EMAIL_USERNAME: 'vbouvier.redactrice@gmail.com'
  EMAIL_PASSWORD: 'avrf enpz otim qlnf'
  NOTIFICATION_EMAIL: 'vbouvier@cybercite.fr'

jobs:
  # ========================================
  # CONFIGURATION DES SITES
  # ========================================
  setup:
    name: ðŸ“‹ Configuration
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      sites_count: ${{ steps.set-matrix.outputs.count }}
    
    steps:
    - name: Configure sites matrix
      id: set-matrix
      run: |
        SITES_JSON=$(cat << 'EOF'
        {
          "sites": [
            {
              "name": "Brown Shipley",
              "url": "https://brownshipley.com/",
              "robots_path": "robots.txt",
              "sitemaps": ["en-gb/sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Orange Caraibe",
              "url": "https://caraibe.orange.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Recrutement FHSJ",
              "url": "https://recrutement-fhsj.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "Assuropoil",
              "url": "https://www.assuropoil.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "Avoriaz",
              "url": "https://www.avoriaz.com/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Domicile Clean",
              "url": "https://www.domicile-clean.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Espaces Atypiques",
              "url": "https://www.espaces-atypiques.com/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "Hopital Marie Lannelongue",
              "url": "https://www.hopitalmarielannelongue.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "HPSJ",
              "url": "https://www.hpsj.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "InsingerGilissen",
              "url": "https://www.insingergilissen.nl/",
              "robots_path": "nl-nl/robots.txt",
              "sitemaps": ["nl-nl/sitemap.xml"],
              "enabled": true
            },
            {
              "name": "International Patient Paris",
              "url": "https://www.international-patient-paris.com/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Le Voyage Ã  Nantes",
              "url": "https://www.levoyageanantes.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "Macoretz",
              "url": "https://www.macoretz.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Merck Finck",
              "url": "https://www.merckfinck.de/",
              "robots_path": "robots.txt",
              "sitemaps": ["de-de/sitemap.xml"],
              "enabled": true
            },
            {
              "name": "NDBS",
              "url": "https://www.ndbs.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "Pharmapets BE",
              "url": "https://www.pharmapets.be/",
              "robots_path": "robots.txt",
              "sitemaps": [
                "media/sitemaps/pharmapets_be/sitemap_nl_product.xml",
                "media/sitemaps/pharmapets_be/sitemap_fr_product.xml"
              ],
              "enabled": true
            },
            {
              "name": "Pharmapets NL",
              "url": "https://www.pharmapets.nl/",
              "robots_path": "robots.txt",
              "sitemaps": [
                "media/sitemaps/pharmapets_nl/sitemap_product.xml"
              ],
              "enabled": true
            },
            {
              "name": "Puilaetco",
              "url": "https://www.puilaetco.be/",
              "robots_path": "robots.txt",
              "sitemaps": ["fr-be/sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Quintet COM",
              "url": "https://www.quintet.com/",
              "robots_path": "robots.txt",
              "sitemaps": ["en-gb/sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Quintet LU",
              "url": "https://www.quintet.lu/",
              "robots_path": "robots.txt",
              "sitemaps": ["en-lu/sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Royan Atlantique",
              "url": "https://www.royanatlantique.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap_index.xml"],
              "enabled": true
            },
            {
              "name": "Sayse",
              "url": "https://www.sayse.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Terre de Marins",
              "url": "https://www.terredemarins.fr/",
              "robots_path": "robots.txt",
              "sitemaps": ["1_fr_0_sitemap.xml"],
              "enabled": true
            },
            {
              "name": "Tom & Co",
              "url": "https://www.tomandco.com/",
              "robots_path": "robots.txt",
              "sitemaps": [
                "media/sitemaps/tomandco/sitemap_tc_benl_product.xml",
                "media/sitemaps/tomandco/sitemap_tc_befr_product.xml"
              ],
              "enabled": true
            },
            {
              "name": "Vetostore",
              "url": "https://www.vetostore.com/",
              "robots_path": "robots.txt",
              "sitemaps": [
                "media/sitemaps/vetostore/sitemap_product.xml"
              ],
              "enabled": true
            },
            {
              "name": "Lea Nature",
              "url": "https://www.leanature.com/",
              "robots_path": "robots.txt",
              "sitemaps": ["media/google_sitemap_3.xml"],
              "enabled": true
            }
          ]
        }
        EOF
        )
        
        if [ -n "${{ github.event.inputs.site_filter }}" ]; then
          FILTERED=$(echo "$SITES_JSON" | jq -c --arg filter "${{ github.event.inputs.site_filter }}" '.sites | map(select(.name | test($filter; "i")) | select(.enabled == true))')
        else
          FILTERED=$(echo "$SITES_JSON" | jq -c '.sites | map(select(.enabled == true))')
        fi
        
        echo "matrix={\"include\":$FILTERED}" >> $GITHUB_OUTPUT
        COUNT=$(echo "$FILTERED" | jq '. | length')
        echo "count=$COUNT" >> $GITHUB_OUTPUT
        echo "ðŸ“Š $COUNT sites configurÃ©s"

  # ========================================
  # MONITORING PARALLÃˆLE
  # ========================================
  monitor:
    name: ðŸ” ${{ matrix.name }}
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      max-parallel: 5
      fail-fast: false
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Setup
      run: |
        SAFE_NAME=$(echo "${{ matrix.name }}" | sed 's/[^a-zA-Z0-9]/_/g' | tr '[:upper:]' '[:lower:]')
        echo "SAFE_NAME=${SAFE_NAME}" >> $GITHUB_ENV
        mkdir -p "data/${SAFE_NAME}"
        mkdir -p "history/${SAFE_NAME}"
        mkdir -p "changes/${SAFE_NAME}"
    
    - name: Download robots.txt
      id: robots
      run: |
        URL="${{ matrix.url }}${{ matrix.robots_path }}"
        echo "ðŸ“¥ Robots.txt: $URL"
        
        HTTP_STATUS=$(curl -s -o "data/${SAFE_NAME}/robots.txt" -w "%{http_code}" -L "$URL" \
          -H "User-Agent: Mozilla/5.0 (compatible; SEOMonitor/1.0)" \
          --max-time 30)
        
        if [ "$HTTP_STATUS" -eq 200 ]; then
          echo "âœ… OK ($(wc -l < data/${SAFE_NAME}/robots.txt) lignes)"
          echo "status=success" >> $GITHUB_OUTPUT
        else
          echo "âŒ Erreur HTTP $HTTP_STATUS"
          echo "status=error" >> $GITHUB_OUTPUT
          echo "error=HTTP $HTTP_STATUS" >> $GITHUB_OUTPUT
        fi
    
    - name: Download sitemaps
      id: sitemaps
      run: |
        echo "ðŸ“¥ TÃ©lÃ©chargement des sitemaps..."
        ALL_SUCCESS=true
        ERRORS=""
        
        echo '${{ toJson(matrix.sitemaps) }}' | jq -r '.[]' | while IFS= read -r sitemap; do
          URL="${{ matrix.url }}${sitemap}"
          FILENAME=$(echo "$sitemap" | sed 's/\//_/g')
          
          echo "  â†’ $URL"
          HTTP_STATUS=$(curl -s -o "data/${SAFE_NAME}/sitemap_${FILENAME}" -w "%{http_code}" -L "$URL" \
            -H "User-Agent: Mozilla/5.0 (compatible; SEOMonitor/1.0)" \
            --max-time 30)
          
          if [ "$HTTP_STATUS" -eq 200 ]; then
            echo "    âœ… OK ($(wc -c < data/${SAFE_NAME}/sitemap_${FILENAME}) bytes)"
          else
            echo "    âŒ Erreur HTTP $HTTP_STATUS"
            ALL_SUCCESS=false
            ERRORS="${ERRORS}${sitemap}:${HTTP_STATUS};"
          fi
        done
        
        if [ "$ALL_SUCCESS" = true ]; then
          echo "status=success" >> $GITHUB_OUTPUT
        else
          echo "status=error" >> $GITHUB_OUTPUT
          echo "errors=$ERRORS" >> $GITHUB_OUTPUT
        fi
    
    - name: Restore history
      id: restore
      uses: actions/download-artifact@v4
      with:
        name: history-${{ env.SAFE_NAME }}
        path: history/${{ env.SAFE_NAME }}/
      continue-on-error: true
    
    - name: Check for changes
      id: changes
      run: |
        echo "ðŸ” Analyse des changements..."
        
        IS_FIRST_RUN=false
        HAS_CHANGES=false
        CHANGES_DETAILS=""
        
        if [ "${{ steps.restore.outcome }}" != "success" ] || [ ! -f "history/${SAFE_NAME}/robots.txt" ]; then
          IS_FIRST_RUN=true
          echo "ðŸ†• PremiÃ¨re vÃ©rification pour ce site"
        fi
        
        if [ -f "data/${SAFE_NAME}/robots.txt" ]; then
          if [ "$IS_FIRST_RUN" = false ] && [ -f "history/${SAFE_NAME}/robots.txt" ]; then
            if ! diff -q "history/${SAFE_NAME}/robots.txt" "data/${SAFE_NAME}/robots.txt" > /dev/null; then
              HAS_CHANGES=true
              echo "ðŸ“ Changements dÃ©tectÃ©s dans robots.txt"
              
              diff -u "history/${SAFE_NAME}/robots.txt" "data/${SAFE_NAME}/robots.txt" > "changes/${SAFE_NAME}/robots_diff.txt" || true
              
              ADDED_LINES=$(grep "^+" "changes/${SAFE_NAME}/robots_diff.txt" | grep -v "^+++" | wc -l || echo 0)
              REMOVED_LINES=$(grep "^-" "changes/${SAFE_NAME}/robots_diff.txt" | grep -v "^---" | wc -l || echo 0)
              
              CHANGES_DETAILS="${CHANGES_DETAILS}robots.txt:modified($ADDED_LINES+/$REMOVED_LINES-);"
            fi
          fi
        fi
        
        for sitemap_file in data/${SAFE_NAME}/sitemap_*; do
          if [ -f "$sitemap_file" ]; then
            BASENAME=$(basename "$sitemap_file")
            
            if [ "$IS_FIRST_RUN" = false ] && [ -f "history/${SAFE_NAME}/$BASENAME" ]; then
              if ! diff -q "$sitemap_file" "history/${SAFE_NAME}/$BASENAME" > /dev/null; then
                HAS_CHANGES=true
                echo "ðŸ“ Changements dans $BASENAME"
                CHANGES_DETAILS="${CHANGES_DETAILS}${BASENAME}:modified;"
              fi
            fi
          fi
        done
        
        if [ "$IS_FIRST_RUN" = true ]; then
          echo "is_first_run=true" >> $GITHUB_OUTPUT
          echo "has_changes=false" >> $GITHUB_OUTPUT
        else
          echo "is_first_run=false" >> $GITHUB_OUTPUT
          echo "has_changes=$HAS_CHANGES" >> $GITHUB_OUTPUT
        fi
        echo "details=$CHANGES_DETAILS" >> $GITHUB_OUTPUT
    
    - name: Create report
      id: report
      run: |
        cat << EOF > "data/${SAFE_NAME}/report.json"
        {
          "site": "${{ matrix.name }}",
          "url": "${{ matrix.url }}",
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "is_first_run": ${{ steps.changes.outputs.is_first_run }},
          "robots": {
            "status": "${{ steps.robots.outputs.status }}",
            "error": "${{ steps.robots.outputs.error }}"
          },
          "sitemaps": {
            "status": "${{ steps.sitemaps.outputs.status }}",
            "errors": "${{ steps.sitemaps.outputs.errors }}"
          },
          "changes": {
            "has_changes": ${{ steps.changes.outputs.has_changes }},
            "details": "${{ steps.changes.outputs.details }}"
          }
        }
        EOF
    
    - name: Save history
      if: always()
      run: |
        if [ -f "data/${SAFE_NAME}/robots.txt" ] && [ "${{ steps.robots.outputs.status }}" = "success" ]; then
          cp "data/${SAFE_NAME}/robots.txt" "history/${SAFE_NAME}/robots.txt"
        fi
        
        for sitemap in data/${SAFE_NAME}/sitemap_*; do
          if [ -f "$sitemap" ]; then
            cp "$sitemap" "history/${SAFE_NAME}/"
          fi
        done
    
    - name: Upload history
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: history-${{ env.SAFE_NAME }}
        path: history/${{ env.SAFE_NAME }}/
        retention-days: 30
    
    - name: Upload report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: report-${{ env.SAFE_NAME }}
        path: data/${{ env.SAFE_NAME }}/report.json
        retention-days: 7

  # [Gardez la section notify comme dans ma rÃ©ponse prÃ©cÃ©dente]
